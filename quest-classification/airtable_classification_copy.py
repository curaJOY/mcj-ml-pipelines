# -*- coding: utf-8 -*-
"""Airtable Classification-copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VTts-QQn1Agvp6cLtxYfkd_yDzkb-2BH

In this notebook, we consider the situation that  columns like 'activities' and 'concerns' hold multiple values at the same time
# Data Preprocessing

- Step 1. Load data and filter out empty column

TODO:
- Add missing columns for the generalizabity test in 4th model. (Ke)
- Create incomplete data by erasing values randomly. Then test model 1, 3, (4) on incomplete data. (Temi)
"""


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, classification_report
from sklearn.svm import SVC
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

quests = pd.read_csv('/content/Quests-All Quests.csv')
quests_updated = quests[['Quests Name','Description','Activities','Associated Behaviors','Linked Concerns','Goal Definition']]



"""- Step 2. Data Augmentation"""

import nltk
from nltk.corpus import wordnet
import random

nltk.download('wordnet')
nltk.download('punkt')

def synonym_replacement(sentence, n=1):
    words = nltk.word_tokenize(sentence)
    new_words = words.copy()
    for _ in range(n):
        for i, word in enumerate(words):
            synsets = wordnet.synsets(word)
            if synsets:
                synonym = random.choice(synsets).lemma_names()[0]
                new_words[i] = synonym
    return ' '.join(new_words)

def random_insertion(sentence, n=1):
    words = nltk.word_tokenize(sentence)
    new_words = words.copy()
    for _ in range(n):
        word = random.choice(words)
        index = random.randint(0, len(new_words) - 1)
        new_words.insert(index, word)
    return ' '.join(new_words)

def random_deletion(sentence, p=0.1):
    words = nltk.word_tokenize(sentence)
    new_words = []
    for word in words:
        if random.uniform(0, 1) > p:
            new_words.append(word)
    return ' '.join(new_words)

def random_swap(sentence, n=1):
    words = nltk.word_tokenize(sentence)
    new_words = words.copy()
    for _ in range(n):
        idx1, idx2 = random.sample(range(len(words)), 2)
        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]
    return ' '.join(new_words)

augmented_data = []
for _, row in quests_updated.iterrows():
    original_description = row['Description']
    augmented_description = [
        synonym_replacement(original_description),
        random_insertion(original_description),
        random_deletion(original_description),
        random_swap(original_description)
    ]
    original_goal_definition = row['Goal Definition']
    augmented_goal_definition = [
        synonym_replacement(original_goal_definition),
        random_insertion(original_goal_definition),
        random_deletion(original_goal_definition),
        random_swap(original_goal_definition)
    ]
    for augmented_desc, augmented_goal in zip(augmented_description, augmented_goal_definition):
        augmented_row = row.copy()
        augmented_row['Description'] = augmented_desc
        augmented_row['Goal Definition'] = augmented_goal
        augmented_data.append(augmented_row)

augmented_df = pd.DataFrame(augmented_data)

combined_data = pd.concat([quests_updated, augmented_df], ignore_index=True)

combined_data.head()

combined_data.fillna("", inplace = True)

combined_data.head()

"""# Data Prepare for Generalizability Test"""

new_data = pd.read_csv('/content/Activities-Linking.csv')

new_data.head()

new_data = new_data[['Activity Name','Quests (Goals)','Concerns','Related Behaviors']]
new_column_names = {'Activity Name': 'Activities', 'Quests (Goals)': 'Quests Name', 'Concerns': 'Linked_concerns', 'Related Behaviors':'Associated_behaviors'}
new_data.rename(columns=new_column_names, inplace=True)
new_data.fillna("", inplace = True)

new_data.head()

combined_quest_names = set(combined_data['Quests Name'])

valid_rows = new_data['Quests Name'].isin(combined_quest_names)

generalize_test_df = new_data[valid_rows]

generalize_test_df.to_csv('/content/filtered_activities.csv', index = False)

generalize_test_X = generalize_test_df['Activities'] + ' ' + generalize_test_df['Linked_concerns'] + ' ' + generalize_test_df['Associated_behaviors']

generalize_test_X_with_cue ='Activities are '+ generalize_test_df['Activities'] + '## Linked Concerns are' + generalize_test_df['Linked_concerns'] + ' ##Associated Behaviors are' + generalize_test_df['Associated_behaviors']

def generalize_test(model, X_input, generalize_test_df = generalize_test_df):
  new_predictions = model.predict(X_input)
  output_df = generalize_test_df.copy()
  output_df['Predicted Quests'] = new_predictions
  generalize_accuracy = accuracy_score(generalize_test_df['Quests Name'], new_predictions)
  print('Test Acc on Generalizability: ', generalize_accuracy)
  return output_df

"""# Model Tranining

## concatenation + tfidf + random forest
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import KFold
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import accuracy_score


X = combined_data['Description'] + ' ' + combined_data['Activities'] + \
    ' ' + combined_data['Associated Behaviors'] + ' ' + combined_data['Linked Concerns'] + ' ' + combined_data[
        'Goal Definition']
y = combined_data['Quests Name']

model_1 = make_pipeline(
    TfidfVectorizer(max_features=1000),
    SelectFromModel(RandomForestClassifier(n_estimators=100)),
    RandomForestClassifier(n_estimators=100)
)

kf = KFold(n_splits=5, shuffle=True, random_state=42)
test_accuracies = []
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    model_1.fit(X_train, y_train)
    y_pred = model_1.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    test_accuracies.append(test_accuracy)

print("Average Test Accuracy:", sum(test_accuracies) / len(test_accuracies))

model_1_gen_df = generalize_test(model_1, X_input = generalize_test_X)




