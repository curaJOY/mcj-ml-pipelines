# Traditional Machine Learning Approach

## ğŸ“Š Overview
This folder contains the implementation of traditional machine learning models for cyberbullying detection. These models serve as our baseline and, surprisingly, achieved the best performance in our comprehensive evaluation.

## ğŸ¯ Methodology

### Feature Engineering
- **1,018 Total Features**: 18 linguistic features + 1,000 bag-of-words features
- **Linguistic Features**: 
  - Length metrics (character count, word count, average word length)
  - Punctuation patterns (exclamation marks, question marks, dots, commas)
  - Capitalization analysis (uppercase count, ratio, all-caps words)
  - Offensive language detection (22 predefined words)
  - Emotional indicators (laughter, crying patterns)
  - Special character analysis

### Models Implemented
1. **Logistic Regression** - Best performing model
2. **Gradient Boosting Classifier** - Second best
3. **Naive Bayes (Multinomial)** - Simple baseline
4. **Support Vector Machine (RBF & Linear)** - Kernel methods
5. **Random Forest** - Ensemble method

### Class Imbalance Handling
- **Balanced class weights** for all compatible models
- **Stratified cross-validation** to maintain class distribution
- **F1-score optimization** prioritized over accuracy

## ğŸ† Results

### Performance Ranking
| Model | F1-Score | Accuracy | Precision | Recall | ROC-AUC |
|-------|----------|----------|-----------|--------|---------|
| **ğŸ¥‡ Logistic Regression** | **0.432** | 84.4% | 40.0% | 47.1% | 0.877 |
| ğŸ¥ˆ Gradient Boosting | 0.414 | 87.4% | 50.0% | 35.3% | 0.835 |
| ğŸ¥‰ Naive Bayes | 0.240 | 85.9% | 37.5% | 17.6% | 0.716 |
| SVM (RBF) | 0.125 | 68.9% | 9.7% | 17.6% | 0.509 |
| Random Forest | 0.000 | 86.7% | 0.0% | 0.0% | 0.843 |

### Best Model Analysis: Logistic Regression
- **Cross-Validation**: 5-fold CV F1 = 0.411 Â± 0.129
- **Confusion Matrix**: TP:11, FP:32, TN:86, FN:6
- **Strong Feature Discrimination**: Offensive word count (coefficient: 1.705) is the strongest predictor

## ğŸ” Key Insights

### What Worked Exceptionally Well
1. **Offensive Language Detection**: Single most powerful predictor
2. **Political Content Correlation**: Unexpected strong link to cyberbullying
3. **Simple Linear Separability**: Task is fundamentally pattern matching
4. **Class Balancing**: Essential for handling 12.7% minority class

### Top Discriminative Features
1. `offensive_word_count` (1.705) - ğŸ¯ **Strongest predictor**
2. `bow_liberal` (1.543) - ğŸ›ï¸ Political term correlation
3. `bow_side` (1.430) - ğŸ’¬ Divisive language indicator
4. `bow_people` (1.329) - ğŸ‘¥ Generalizing attack language
5. `bow_what` (1.171) - â“ Interrogative aggression

## ğŸ’¡ Why Traditional ML Dominates

### Dataset Characteristics
- **Small Scale**: 674 samples (traditional ML sweet spot)
- **Feature Rich**: Well-engineered features capture key patterns
- **Linear Patterns**: Cyberbullying detection is primarily pattern matching
- **Class Imbalance**: Simple models handle 12.7% minority class better

### Advantages Over Complex Models
- **No Overfitting**: Simple models generalize better with limited data
- **Fast Training**: Quick iteration and experimentation
- **Interpretable**: Clear feature importance and decision boundaries
- **Production Ready**: Low latency, easy deployment

## ğŸš€ Usage

```python
from traditional_ml_models import CyberbullyingDetector

# Initialize and train the best model
detector = CyberbullyingDetector()
detector.train_logistic_regression()

# Make predictions
prediction = detector.predict("Your text here")
confidence = detector.get_confidence()
```

## ğŸ“ Files
- `traditional_ml_models.py` - Complete implementation of all traditional ML models
- `README.md` - This documentation

## ğŸ¯ Future Improvements
- **Feature enhancement**: Better offensive word dictionaries
- **Ensemble methods**: Combine multiple traditional models
- **Context features**: Add relationship and conversation context
- **Real-time optimization**: Sub-millisecond inference for production 