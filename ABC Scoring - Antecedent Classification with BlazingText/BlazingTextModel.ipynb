{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433492f2",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3819e933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Antecedents</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i asked my husband to please put away the laun...</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>told aiden to wash his hands</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was time to clean up their toys</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jack was stomping his feet and i asked him to ...</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>she had to write a sentence about her day. wri...</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Antecedents  \\\n",
       "0  i asked my husband to please put away the laun...   \n",
       "1                       told aiden to wash his hands   \n",
       "2                 It was time to clean up their toys   \n",
       "3  jack was stomping his feet and i asked him to ...   \n",
       "4  she had to write a sentence about her day. wri...   \n",
       "\n",
       "                                              Labels  \n",
       "0  They were given directions or a task to comple...  \n",
       "1  They were given directions or a task to comple...  \n",
       "2  They were given directions or a task to comple...  \n",
       "3  They were given directions or a task to comple...  \n",
       "4  They were given directions or a task to comple...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "training_data_path = 'ABC Training Data-Grid view.csv'\n",
    "labels_data_path = 'Antecedents- labels.csv'\n",
    "\n",
    "training_data = pd.read_csv(training_data_path)\n",
    "labels_data = pd.read_csv(labels_data_path)\n",
    "\n",
    "training_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c843376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They were given directions or a task to complete</td>\n",
       "      <td>Antecedent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They were in the middle of a long task or assi...</td>\n",
       "      <td>Antecedent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Given a difficult, unclear, or challenging tas...</td>\n",
       "      <td>Antecedent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They were in the middle of something they enjo...</td>\n",
       "      <td>Antecedent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Someone corrected or helped them</td>\n",
       "      <td>Antecedent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name        Type\n",
       "0   They were given directions or a task to complete  Antecedent\n",
       "1  They were in the middle of a long task or assi...  Antecedent\n",
       "2  Given a difficult, unclear, or challenging tas...  Antecedent\n",
       "3  They were in the middle of something they enjo...  Antecedent\n",
       "4                   Someone corrected or helped them  Antecedent"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca7871ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b909028",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3564acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: i asked my husband to please put away the laundry and he did what he always does\n",
      "Augmented: ace asked my husband to please set away the washables and he come what he forever does\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert the part-of-speech naming scheme\n",
    "       from the nltk default to that which is recognized by the WordNet API\"\"\"\n",
    "    return {\n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }.get(treebank_tag[0], wordnet.NOUN)  # Default to noun if part-of-speech is not found\n",
    "\n",
    "def synonym_replacement(sentence, num_replacements=1):\n",
    "    # Tokenize and POS tag the words in the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Get synonyms for each word, considering its part of speech\n",
    "    synonyms = defaultdict(list)\n",
    "    for word, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)  # Convert to WordNet POS notation\n",
    "        for syn in wordnet.synsets(word, pos=wordnet_pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonym = lemma.name().replace('_', ' ').replace('-', ' ')\n",
    "                if synonym != word:\n",
    "                    synonyms[word].append(synonym)\n",
    "\n",
    "    # Select random words to replace\n",
    "    words_to_replace = random.sample(list(synonyms.keys()), min(num_replacements, len(synonyms)))\n",
    "\n",
    "    # Perform replacements\n",
    "    new_sentence = sentence\n",
    "    for word in words_to_replace:\n",
    "        syn_list = synonyms[word]\n",
    "        if syn_list:\n",
    "            # Choose a random synonym for the word\n",
    "            synonym = random.choice(syn_list)\n",
    "            new_sentence = new_sentence.replace(word, synonym, 1)\n",
    "\n",
    "    return new_sentence\n",
    "\n",
    "# Test the function\n",
    "original_text = \"i asked my husband to please put away the laundry and he did what he always does\"\n",
    "augmented_text = synonym_replacement(original_text, num_replacements=5)\n",
    "print(\"Original:\", original_text)\n",
    "print(\"Augmented:\", augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "219f6443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1032"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def augment_sentences(dataframe, augment_factor=5):\n",
    "    augmented_rows = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        text, label = row['Antecedents'], row['Labels']\n",
    "        unique_augmented_texts = set()\n",
    "        while len(unique_augmented_texts) < augment_factor:\n",
    "            augmented_text = synonym_replacement(text, num_replacements=3)\n",
    "            unique_augmented_texts.add(augmented_text)\n",
    "        for aug_text in unique_augmented_texts:\n",
    "            augmented_rows.append([aug_text, label])\n",
    "    return augmented_rows\n",
    "\n",
    "\n",
    "\n",
    "augmented_data = augment_sentences(training_data, augment_factor=5)\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data, columns=['Antecedents', 'Labels'])\n",
    "\n",
    "data = pd.concat([training_data[['Antecedents', 'Labels']], augmented_df])\n",
    "\n",
    "data = combined_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca27205",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da87bc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3e431e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i asked my husband to please put away the laun...\n",
       "1                         told aiden to wash his hands\n",
       "2                   it was time to clean up their toys\n",
       "3    jack was stomping his feet and i asked him to ...\n",
       "4    she had to write a sentence about her day writ...\n",
       "Name: antecedents_clean, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Join tokens back into string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['antecedents_clean'] = data['Antecedents'].apply(clean_and_tokenize)\n",
    "data['antecedents_clean'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee62f2",
   "metadata": {},
   "source": [
    "## Format the Data for Blazing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6398969",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels_formatted'] = data['Labels'].apply(lambda x: ' '.join(['__label__' + label.strip() for label in x.split(',')]))\n",
    "\n",
    "data['blazingtext_format'] = data['antecedents_clean'] + \" \" + data['labels_formatted']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004b8bd3",
   "metadata": {},
   "source": [
    "## Split Training/Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff6c2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, validation = train_test_split(data['blazingtext_format'], test_size=0.2, random_state=42) \n",
    "\n",
    "train.to_csv(\"train_data.txt\", index=False, header=False)\n",
    "validation.to_csv(\"validation_data.txt\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537de7f",
   "metadata": {},
   "source": [
    "## Upload Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "825246fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data location: s3://abc-scoring-blazingtexts-trainingdata/sagemaker/antecedents-classification/training/train_data.txt\n",
      "Validation data location: s3://abc-scoring-blazingtexts-trainingdata/sagemaker/antecedents-classification/validation/validation_data.txt\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = 'abc-scoring-blazingtexts-trainingdata'\n",
    "prefix = 'sagemaker/antecedents-classification'\n",
    "\n",
    "train_data = sagemaker_session.upload_data(path=\"train_data.txt\", bucket=bucket, key_prefix=f\"{prefix}/training\")\n",
    "validation_data = sagemaker_session.upload_data(path=\"validation_data.txt\", bucket=bucket, key_prefix=f\"{prefix}/validation\")\n",
    "\n",
    "print(f'Training data location: {train_data}')\n",
    "print(f'Validation data location: {validation_data}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a031e93b",
   "metadata": {},
   "source": [
    "## Set Up and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e05db9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "WARNING:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: blazingtext-2024-04-14-21-55-27-493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-14 21:55:27 Starting - Starting the training job...\n",
      "2024-04-14 21:55:43 Starting - Preparing the instances for training......\n",
      "2024-04-14 21:56:33 Downloading - Downloading input data...\n",
      "2024-04-14 21:57:28 Training - Training image download completed. Training in progress...\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[04/14/2024 21:57:31 WARNING 140170336032576] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[04/14/2024 21:57:31 WARNING 140170336032576] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[04/14/2024 21:57:31 INFO 140170336032576] nvidia-smi took: 0.025162935256958008 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[04/14/2024 21:57:31 INFO 140170336032576] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34mNumber of CPU sockets found in instance is  1\u001b[0m\n",
      "\u001b[34m[04/14/2024 21:57:31 INFO 140170336032576] Processing /opt/ml/input/data/train/train_data.txt . File size: 0.16261005401611328 MB\u001b[0m\n",
      "\u001b[34m[04/14/2024 21:57:31 INFO 140170336032576] Processing /opt/ml/input/data/validation/validation_data.txt . File size: 0.04179954528808594 MB\u001b[0m\n",
      "\u001b[34mRead 0M words\u001b[0m\n",
      "\u001b[34mNumber of words:  544\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/validation_data.txt\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0398  Progress: 20.32%  Million Words/sec: 1.85 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.347826\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0303  Progress: 39.40%  Million Words/sec: 0.26 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 19\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.531401\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0242  Progress: 51.66%  Million Words/sec: 0.18 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 25\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.642512\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0178  Progress: 64.31%  Million Words/sec: 0.15 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 32\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.642512\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0120  Progress: 76.07%  Million Words/sec: 0.17 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 38\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.642512\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0053  Progress: 89.36%  Million Words/sec: 0.20 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 44\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.642512\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 3 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0002  Progress: 99.58%  Million Words/sec: 0.21 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 49\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.642512\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 4 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 50\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.642512\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 5 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 0.21 #####\u001b[0m\n",
      "\n",
      "2024-04-14 21:57:54 Uploading - Uploading generated training model\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 0.21\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 4.80\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.6315\u001b[0m\n",
      "\u001b[34mNumber of train examples: 825\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.6425\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 207\u001b[0m\n",
      "\n",
      "2024-04-14 21:59:09 Completed - Training job completed\n",
      "Training seconds: 156\n",
      "Billable seconds: 156\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "container = get_image_uri(boto3.Session().region_name, 'blazingtext', 'latest')\n",
    "\n",
    "blazingtext = sagemaker.estimator.Estimator(container,\n",
    "                                            role,\n",
    "                                            instance_count=1,\n",
    "                                            instance_type='ml.m5.large',\n",
    "                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "blazingtext.set_hyperparameters(mode=\"supervised\",\n",
    "                                epochs=50,  # Increased from 10 to 50\n",
    "                                learning_rate=0.05,  # Adjusted learning rate\n",
    "                                vector_dim=200,  # Increased vector dimensions\n",
    "                                early_stopping=True,\n",
    "                                patience=10,  # Increased patience\n",
    "                                min_epochs=10,  # Increased min epochs\n",
    "                                word_ngrams=3)  # Trying higher n-grams\n",
    "\n",
    "blazingtext.fit({'train': train_data, 'validation': validation_data})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f1cec",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "675795c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: blazingtext-240414-2203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'epochs': IntegerParameter(10, 100),\n",
    "    'learning_rate': ContinuousParameter(0.01, 0.1),\n",
    "    'vector_dim': IntegerParameter(100, 300),\n",
    "    'word_ngrams': IntegerParameter(1, 5),\n",
    "}\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=blazingtext,\n",
    "    objective_metric_name='validation:accuracy',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=3,\n",
    "    objective_type='Maximize'\n",
    ")\n",
    "\n",
    "# Start hyperparameter tuning\n",
    "tuner.fit({'train': train_data, 'validation': validation_data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e02316df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "WARNING:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: blazingtext-2024-04-14-22-28-33-119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-14 22:28:33 Starting - Starting the training job...\n",
      "2024-04-14 22:28:48 Starting - Preparing the instances for training......\n",
      "2024-04-14 22:29:44 Downloading - Downloading input data...\n",
      "2024-04-14 22:30:29 Downloading - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[04/14/2024 22:30:43 WARNING 140234246543168] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[04/14/2024 22:30:43 WARNING 140234246543168] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[04/14/2024 22:30:43 INFO 140234246543168] nvidia-smi took: 0.025173664093017578 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[04/14/2024 22:30:43 INFO 140234246543168] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34mNumber of CPU sockets found in instance is  1\u001b[0m\n",
      "\u001b[34m[04/14/2024 22:30:43 INFO 140234246543168] Processing /opt/ml/input/data/train/train_data.txt . File size: 0.16261005401611328 MB\u001b[0m\n",
      "\u001b[34m[04/14/2024 22:30:43 INFO 140234246543168] Processing /opt/ml/input/data/validation/validation_data.txt . File size: 0.04179954528808594 MB\u001b[0m\n",
      "\u001b[34mRead 0M words\u001b[0m\n",
      "\u001b[34mNumber of words:  544\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/validation_data.txt\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0797  Progress: 16.86%  Million Words/sec: 3.34 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 16\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.816425\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 18\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.859903\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0680  Progress: 29.05%  Million Words/sec: 1.85 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 29\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.898551\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0576  Progress: 39.88%  Million Words/sec: 1.92 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 39\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.971014\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0473  Progress: 50.63%  Million Words/sec: 1.96 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 50\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.975845\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0371  Progress: 61.30%  Million Words/sec: 1.99 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 61\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.966184\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0267  Progress: 72.09%  Million Words/sec: 2.01 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 72\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.980676\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0166  Progress: 82.66%  Million Words/sec: 2.02 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 82\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.975845\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0063  Progress: 93.47%  Million Words/sec: 2.04 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 93\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.975845\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: -0.0000  Progress: 100.00%  Million Words/sec: 1.96 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 100\u001b[0m\n",
      "\u001b[34mUsing 2 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.980676\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 3 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 1.93 #####\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 1.93\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 1.04\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.9903\u001b[0m\n",
      "\u001b[34mNumber of train examples: 825\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.9807\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 207\u001b[0m\n",
      "\n",
      "2024-04-14 22:30:55 Training - Training image download completed. Training in progress.\n",
      "2024-04-14 22:30:55 Uploading - Uploading generated training model\n",
      "2024-04-14 22:31:10 Completed - Training job completed\n",
      "Training seconds: 87\n",
      "Billable seconds: 87\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "container = get_image_uri(boto3.Session().region_name, 'blazingtext', 'latest')\n",
    "\n",
    "blazingtext = sagemaker.estimator.Estimator(container,\n",
    "                                            role,\n",
    "                                            instance_count=1,\n",
    "                                            instance_type='ml.m5.large',\n",
    "                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Setting hyperparameters based on the best training job results\n",
    "blazingtext.set_hyperparameters(mode=\"supervised\",\n",
    "                                epochs=100,\n",
    "                                learning_rate=0.09582345631158493, \n",
    "                                vector_dim=120, \n",
    "                                early_stopping=True, \n",
    "                                patience=10,  \n",
    "                                min_epochs=10,  \n",
    "                                word_ngrams=1)  \n",
    "\n",
    "blazingtext.fit({'train': train_data, 'validation': validation_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f5b98",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "204059f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: blazingtext-2024-04-14-22-38-15-590\n",
      "INFO:sagemaker:Creating endpoint-config with name blazingtext-2024-04-14-22-38-16-116\n",
      "INFO:sagemaker:Creating endpoint with name blazingtext-2024-04-14-22-38-16-116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!Endpoint Name: blazingtext-2024-04-14-22-38-16-116\n"
     ]
    }
   ],
   "source": [
    "blazingtext_antecedents_model = blazingtext.create_model()\n",
    "\n",
    "predictor = blazingtext_antecedents_model.deploy(initial_instance_count=1,\n",
    "                                     instance_type='ml.m4.xlarge')\n",
    "\n",
    "\n",
    "# Get the endpoint name\n",
    "endpoint_name = predictor.endpoint_name\n",
    "print(f\"Endpoint Name: {endpoint_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a31019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
