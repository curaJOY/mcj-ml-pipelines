{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ca179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numexpr in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.7.3)\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from numexpr) (1.22.4)\n",
      "Downloading numexpr-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (375 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.2/375.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numexpr\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.7.3\n",
      "    Uninstalling numexpr-2.7.3:\n",
      "      Successfully uninstalled numexpr-2.7.3\n",
      "Successfully installed numexpr-2.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd07a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e1ba9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8646dc",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa6015a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths to data/label\n",
    "training_data_path = 'ABC Training Data-Grid view.csv'\n",
    "labels_data_path = 'Antecedents- labels.csv'\n",
    "\n",
    "#Load data\n",
    "antecedents_data = pd.read_csv(training_data_path)\n",
    "labels_data = pd.read_csv(labels_data_path)\n",
    "\n",
    "#Read in data\n",
    "texts = antecedents_data['Texts']\n",
    "labels = antecedents_data['Labels'].apply(lambda x: x.split(','))  # labels are comma-separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3263806d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(antecedents_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bc6986e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texts</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Hypo. Function 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i asked my husband to please put away the laun...</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "      <td>Escape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>told aiden to wash his hands</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "      <td>Escape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was time to clean up their toys</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "      <td>Escape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jack was stomping his feet and i asked him to ...</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "      <td>Escape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>she had to write a sentence about her day. wri...</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "      <td>Escape</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Texts  \\\n",
       "0  i asked my husband to please put away the laun...   \n",
       "1                       told aiden to wash his hands   \n",
       "2                 It was time to clean up their toys   \n",
       "3  jack was stomping his feet and i asked him to ...   \n",
       "4  she had to write a sentence about her day. wri...   \n",
       "\n",
       "                                              Labels Hypo. Function 1  \n",
       "0  They were given directions or a task to comple...           Escape  \n",
       "1  They were given directions or a task to comple...           Escape  \n",
       "2  They were given directions or a task to comple...           Escape  \n",
       "3  They were given directions or a task to comple...           Escape  \n",
       "4  They were given directions or a task to comple...           Escape  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antecedents_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638961c0",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fa2ccac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [asked, husband, please, put, away, laundry, a...\n",
       "1                           [told, aiden, wash, hands]\n",
       "2                                  [time, clean, toys]\n",
       "3          [jack, stomping, feet, asked, walk, nicely]\n",
       "4    [write, sentence, day, writing, hard, even, th...\n",
       "Name: Texts, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and make lowercase\n",
    "    tokens = [w.lower() for w in tokens if w.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return tokens\n",
    "\n",
    "preprocessed_texts = texts.apply(preprocess_text)\n",
    "\n",
    "preprocessed_texts.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e88ed8",
   "metadata": {},
   "source": [
    "## Data Augmentation: Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5742fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: i asked my husband to please put away the laundry and he did what he always does\n",
      "Augmented: I necessitate my husband to please position away the laundry and he make what he perpetually does\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert the part-of-speech naming scheme\n",
    "       from the nltk default to that which is recognized by the WordNet API\"\"\"\n",
    "    return {\n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }.get(treebank_tag[0], wordnet.NOUN)  # Default to noun if part-of-speech is not found\n",
    "\n",
    "def synonym_replacement(sentence, num_replacements=1):\n",
    "    # Tokenize and POS tag the words in the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Get synonyms for each word, considering its part of speech\n",
    "    synonyms = defaultdict(list)\n",
    "    for word, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)  # Convert to WordNet POS notation\n",
    "        for syn in wordnet.synsets(word, pos=wordnet_pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonym = lemma.name().replace('_', ' ').replace('-', ' ')\n",
    "                if synonym != word:\n",
    "                    synonyms[word].append(synonym)\n",
    "\n",
    "    # Select random words to replace\n",
    "    words_to_replace = random.sample(list(synonyms.keys()), min(num_replacements, len(synonyms)))\n",
    "\n",
    "    # Perform replacements\n",
    "    new_sentence = sentence\n",
    "    for word in words_to_replace:\n",
    "        syn_list = synonyms[word]\n",
    "        if syn_list:\n",
    "            # Choose a random synonym for the word\n",
    "            synonym = random.choice(syn_list)\n",
    "            new_sentence = new_sentence.replace(word, synonym, 1)\n",
    "\n",
    "    return new_sentence\n",
    "\n",
    "# Test the function\n",
    "original_text = \"i asked my husband to please put away the laundry and he did what he always does\"\n",
    "augmented_text = synonym_replacement(original_text, num_replacements=5)\n",
    "print(\"Original:\", original_text)\n",
    "print(\"Augmented:\", augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46a256ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def augment_sentences(dataframe, augment_factor=5):\n",
    "    augmented_rows = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        text, label = row['Texts'], row['Labels']\n",
    "        unique_augmented_texts = set()\n",
    "        while len(unique_augmented_texts) < augment_factor:\n",
    "            augmented_text = synonym_replacement(text, num_replacements=3)\n",
    "            unique_augmented_texts.add(augmented_text)\n",
    "        for aug_text in unique_augmented_texts:\n",
    "            augmented_rows.append([aug_text, label])\n",
    "    return augmented_rows\n",
    "\n",
    "\n",
    "\n",
    "augmented_data = augment_sentences(antecedents_data, augment_factor=5)\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data, columns=['Texts', 'Labels'])\n",
    "\n",
    "combined_dataset = pd.concat([antecedents_data[['Texts', 'Labels']], augmented_df])\n",
    "\n",
    "combined_dataset = combined_dataset.reset_index(drop=True)\n",
    "\n",
    "#combined_dataset.to_csv('augmented_training_data.csv', index=False)\n",
    "\n",
    "len(combined_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8338007",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63437f41",
   "metadata": {},
   "source": [
    "### 1. Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9aac687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 216 samples\n",
      "Validation set size: 54 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and Labels\n",
    "X = combined_dataset['Texts']  # the features we want to analyze\n",
    "y = combined_dataset['Labels']  # the labels, or answers, we want to test against\n",
    "\n",
    "# Split the data into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the size of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4ef66",
   "metadata": {},
   "source": [
    "### 2. Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "325e29f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training feature vectors shape: (216, 1696)\n",
      "Validation feature vectors shape: (54, 1696)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the training data to compute TF-IDF features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation data to compute TF-IDF features\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "\n",
    "# We can take a look at the shape of the resulting feature vectors\n",
    "print(f\"Training feature vectors shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Validation feature vectors shape: {X_val_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6643c940",
   "metadata": {},
   "source": [
    "### 3. Model training using Random Forest  with Multiouptout Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4321d41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7592592592592593\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the labels into a binary format for multi-label classification\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_mlb = mlb.fit_transform(y_train.apply(lambda x: x.split(',')))\n",
    "y_val_mlb = mlb.transform(y_val.apply(lambda x: x.split(',')))\n",
    "\n",
    "# Initialize the MultiOutputClassifier with RandomForest\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "multi_target_forest = MultiOutputClassifier(forest, n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "multi_target_forest.fit(X_train_tfidf, y_train_mlb)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = multi_target_forest.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val_mlb, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1184a7a6",
   "metadata": {},
   "source": [
    "### 4. Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1720b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best hyperparameters: {'estimator__max_depth': None, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 200}\n",
      "Best cross-validated score: 0.5\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   6.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  10.5s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  10.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  20.4s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   5.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   6.2s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   6.7s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  12.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  22.8s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  22.6s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  15.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  12.9s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  21.9s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   5.6s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   6.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   5.8s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  11.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  20.6s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  21.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  11.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  11.5s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  21.2s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   5.7s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   6.5s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=50; total time=  12.2s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  16.5s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  18.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  23.8s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  11.8s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  11.7s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  22.9s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   6.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   5.7s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   6.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  11.2s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  25.5s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  21.8s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  10.5s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  10.9s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  20.4s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   4.9s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   5.2s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  10.1s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  11.7s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  19.1s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   6.3s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   6.3s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   6.3s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  11.7s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  27.3s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  22.7s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  11.0s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  12.2s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  22.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   5.7s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   6.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  10.5s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  19.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  23.4s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  12.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  11.6s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  22.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   5.2s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   6.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   7.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  15.2s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  22.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  23.2s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  10.6s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  11.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  19.8s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   4.8s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   5.6s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   6.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  10.9s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  21.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  25.2s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  17.4s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  11.4s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  19.8s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   6.2s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   6.4s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   6.6s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  11.5s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  22.5s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  23.7s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  10.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  12.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  26.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   4.9s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   4.9s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   5.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  10.4s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  21.5s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  18.6s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   5.0s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  10.8s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  20.5s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  23.3s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  11.5s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  11.8s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  26.6s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   6.0s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   5.6s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   6.1s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  10.6s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  22.6s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  18.3s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [50, 100, 200],\n",
    "    'estimator__max_depth': [None, 10, 20, 30],\n",
    "    'estimator__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the MultiOutputClassifier and the parameter grid\n",
    "grid_search = GridSearchCV(multi_target_forest, param_grid=param_grid, cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_tfidf, y_train_mlb)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Best cross-validated score\n",
    "print(f\"Best cross-validated score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9777fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       precision    recall  f1-score   support\n",
      "\n",
      "                                                 busy       1.00      1.00      1.00         1\n",
      "                                                 cold       1.00      1.00      1.00         2\n",
      "                                              crowded       1.00      1.00      1.00         1\n",
      "                                               etc.)\"       1.00      1.00      1.00         2\n",
      "                                             on phone       1.00      0.83      0.91         6\n",
      "                        or challenging task/activity\"       1.00      1.00      1.00         3\n",
      "                         or overwhelming environment\"       1.00      1.00      1.00         1\n",
      "                           talking with someone else\"       1.00      0.83      0.91         6\n",
      "                                              unclear       1.00      1.00      1.00         3\n",
      "                                   \"Given a difficult       1.00      1.00      1.00         3\n",
      "                                                \"Loud       1.00      1.00      1.00         1\n",
      "                               \"Parent/caregiver busy       1.00      0.83      0.91         6\n",
      "                            \"Physical discomfort (wet       1.00      1.00      1.00         2\n",
      "          \"They wanted something and got told \"\"no\"\"\"       1.00      1.00      1.00         3\n",
      "                 Another person had an item they want       1.00      0.83      0.91         6\n",
      "              Asked to stop using something they like       1.00      0.29      0.44         7\n",
      "                            Bad day at school or work       1.00      1.00      1.00         1\n",
      "   Belongings disturbed without permission/unexpected       1.00      1.00      1.00         3\n",
      "                            Disagreement with someone       1.00      1.00      1.00         2\n",
      "             Forced participation in activity or task       1.00      1.00      1.00         1\n",
      "           Nobody was really paying attention to them       1.00      0.83      0.91         6\n",
      "                           Nothing to do or play with       1.00      0.50      0.67         2\n",
      "                  Someone got in their personal space       1.00      1.00      1.00         2\n",
      "                            Someone out-competed them       1.00      0.83      0.91         6\n",
      "      Stopping one activity/task and starting another       1.00      0.92      0.96        12\n",
      "                    Sudden change of plans or routine       1.00      1.00      1.00         4\n",
      "                Their choice was not honored/accepted       1.00      0.50      0.67         4\n",
      "                    They wanted something unavailable       1.00      0.75      0.86         8\n",
      "        They were asked or told to wait for something       1.00      1.00      1.00         1\n",
      "     They were given directions or a task to complete       1.00      0.86      0.92        14\n",
      " They were in the middle of a long task or assignment       1.00      1.00      1.00         1\n",
      "They were in the middle of something they enjoy doing       1.00      0.75      0.86        12\n",
      "         While playing in a large group (like recess)       1.00      1.00      1.00         1\n",
      "                 While playing with a sibling or peer       1.00      1.00      1.00         1\n",
      "\n",
      "                                            micro avg       1.00      0.84      0.91       134\n",
      "                                            macro avg       1.00      0.90      0.94       134\n",
      "                                         weighted avg       1.00      0.84      0.90       134\n",
      "                                          samples avg       0.89      0.83      0.85       134\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_forest = RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    max_depth=None, \n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Wrap the classifier with MultiOutputClassifier\n",
    "best_multi_target_forest = MultiOutputClassifier(best_forest, n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "best_multi_target_forest.fit(X_train_tfidf, y_train_mlb)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = best_multi_target_forest.predict(X_val_tfidf)\n",
    "\n",
    "# Detailed performance analysis\n",
    "print(classification_report(y_val_mlb, y_val_pred, target_names=mlb.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d725633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       precision    recall  f1-score   support\n",
      "\n",
      "                                                 busy       1.00      1.00      1.00         1\n",
      "                                                 cold       1.00      1.00      1.00         2\n",
      "                                              crowded       1.00      1.00      1.00         1\n",
      "                                               etc.)\"       1.00      1.00      1.00         2\n",
      "                                             on phone       1.00      0.83      0.91         6\n",
      "                        or challenging task/activity\"       1.00      1.00      1.00         3\n",
      "                         or overwhelming environment\"       1.00      1.00      1.00         1\n",
      "                           talking with someone else\"       1.00      0.83      0.91         6\n",
      "                                              unclear       1.00      1.00      1.00         3\n",
      "                                   \"Given a difficult       1.00      1.00      1.00         3\n",
      "                                                \"Loud       1.00      1.00      1.00         1\n",
      "                               \"Parent/caregiver busy       1.00      0.83      0.91         6\n",
      "                            \"Physical discomfort (wet       1.00      1.00      1.00         2\n",
      "          \"They wanted something and got told \"\"no\"\"\"       1.00      1.00      1.00         3\n",
      "                 Another person had an item they want       1.00      0.83      0.91         6\n",
      "              Asked to stop using something they like       1.00      0.29      0.44         7\n",
      "                            Bad day at school or work       1.00      1.00      1.00         1\n",
      "   Belongings disturbed without permission/unexpected       1.00      1.00      1.00         3\n",
      "                            Disagreement with someone       1.00      1.00      1.00         2\n",
      "             Forced participation in activity or task       1.00      1.00      1.00         1\n",
      "           Nobody was really paying attention to them       1.00      0.83      0.91         6\n",
      "                           Nothing to do or play with       1.00      0.50      0.67         2\n",
      "                  Someone got in their personal space       1.00      1.00      1.00         2\n",
      "                            Someone out-competed them       1.00      0.83      0.91         6\n",
      "      Stopping one activity/task and starting another       1.00      0.92      0.96        12\n",
      "                    Sudden change of plans or routine       1.00      1.00      1.00         4\n",
      "                Their choice was not honored/accepted       1.00      0.50      0.67         4\n",
      "                    They wanted something unavailable       1.00      0.75      0.86         8\n",
      "        They were asked or told to wait for something       1.00      1.00      1.00         1\n",
      "     They were given directions or a task to complete       1.00      0.86      0.92        14\n",
      " They were in the middle of a long task or assignment       1.00      1.00      1.00         1\n",
      "They were in the middle of something they enjoy doing       1.00      0.75      0.86        12\n",
      "         While playing in a large group (like recess)       1.00      1.00      1.00         1\n",
      "                 While playing with a sibling or peer       1.00      1.00      1.00         1\n",
      "\n",
      "                                            micro avg       1.00      0.84      0.91       134\n",
      "                                            macro avg       1.00      0.90      0.94       134\n",
      "                                         weighted avg       1.00      0.84      0.90       134\n",
      "                                          samples avg       0.89      0.83      0.85       134\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Since the best hyperparameters were provided, we'll use those to create a new RandomForest\n",
    "best_forest = RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    max_depth=None, \n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Wrap this forest in a MultiOutputClassifier\n",
    "best_multi_target_forest = MultiOutputClassifier(best_forest, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the full training data\n",
    "best_multi_target_forest.fit(X_train_tfidf, y_train_mlb)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = best_multi_target_forest.predict(X_val_tfidf)\n",
    "\n",
    "# Generate the classification report\n",
    "print(classification_report(y_val_mlb, y_val_pred, target_names=mlb.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa62859",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94c41296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold: [0.7777777777777778, 0.8703703703703703, 0.7592592592592593, 0.7407407407407407, 0.7962962962962963]\n",
      "Mean accuracy across all folds: 0.7888888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare to collect the scores\n",
    "scores = []\n",
    "\n",
    "# Perform the cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split data\n",
    "    X_train_kf, X_val_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_kf, y_val_kf = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Transform the labels\n",
    "    y_train_kf_mlb = mlb.transform(y_train_kf.apply(lambda x: x.split(',')))\n",
    "    y_val_kf_mlb = mlb.transform(y_val_kf.apply(lambda x: x.split(',')))\n",
    "\n",
    "    # Vectorize the text\n",
    "    X_train_kf_tfidf = vectorizer.fit_transform(X_train_kf)\n",
    "    X_val_kf_tfidf = vectorizer.transform(X_val_kf)\n",
    "\n",
    "    # Train the model\n",
    "    best_multi_target_forest.fit(X_train_kf_tfidf, y_train_kf_mlb)\n",
    "\n",
    "    # Predict on the validation fold\n",
    "    y_val_kf_pred = best_multi_target_forest.predict(X_val_kf_tfidf)\n",
    "\n",
    "    # Compute the accuracy for the current fold\n",
    "    accuracy = accuracy_score(y_val_kf_mlb, y_val_kf_pred)\n",
    "\n",
    "    # Append the score\n",
    "    scores.append(accuracy)\n",
    "\n",
    "# Display the accuracy for each fold\n",
    "print(f\"Accuracy for each fold: {scores}\")\n",
    "\n",
    "# Compute the mean accuracy\n",
    "mean_accuracy = np.mean(scores)\n",
    "print(f\"Mean accuracy across all folds: {mean_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
