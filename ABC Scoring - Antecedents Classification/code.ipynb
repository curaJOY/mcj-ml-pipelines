{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ca179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numexpr in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from numexpr) (1.22.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd07a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24e1ba9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8646dc",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6015a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths to data/label\n",
    "training_data_path = 'ABC Training Data-Grid view.csv'\n",
    "labels_data_path = 'Antecedents- labels.csv'\n",
    "\n",
    "#Load data\n",
    "antecedents_data = pd.read_csv(training_data_path)\n",
    "labels_data = pd.read_csv(labels_data_path)\n",
    "\n",
    "#Read in data\n",
    "texts = antecedents_data['Texts']\n",
    "labels = antecedents_data['Labels'].apply(lambda x: x.split(','))  # labels are comma-separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3263806d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(antecedents_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bc6986e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texts</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i asked my husband to please put away the laun...</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>told aiden to wash his hands</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was time to clean up their toys</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jack was stomping his feet and i asked him to ...</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>she had to write a sentence about her day. wri...</td>\n",
       "      <td>They were given directions or a task to comple...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Texts  \\\n",
       "0  i asked my husband to please put away the laun...   \n",
       "1                       told aiden to wash his hands   \n",
       "2                 It was time to clean up their toys   \n",
       "3  jack was stomping his feet and i asked him to ...   \n",
       "4  she had to write a sentence about her day. wri...   \n",
       "\n",
       "                                              Labels  \n",
       "0  They were given directions or a task to comple...  \n",
       "1  They were given directions or a task to comple...  \n",
       "2  They were given directions or a task to comple...  \n",
       "3  They were given directions or a task to comple...  \n",
       "4  They were given directions or a task to comple...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antecedents_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638961c0",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fa2ccac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [asked, husband, please, put, away, laundry, a...\n",
       "1                           [told, aiden, wash, hands]\n",
       "2                                  [time, clean, toys]\n",
       "3          [jack, stomping, feet, asked, walk, nicely]\n",
       "4    [write, sentence, day, writing, hard, even, th...\n",
       "Name: Texts, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and make lowercase\n",
    "    tokens = [w.lower() for w in tokens if w.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return tokens\n",
    "\n",
    "preprocessed_texts = texts.apply(preprocess_text)\n",
    "\n",
    "preprocessed_texts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cc209a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         asked husband please put away laundry always\n",
       "1                                told aiden wash hands\n",
       "2                                      time clean toys\n",
       "3                 jack stomping feet asked walk nicely\n",
       "4    write sentence day writing hard even though tr...\n",
       "Name: Texts, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and make lowercase\n",
    "    tokens = [w.lower() for w in tokens if w.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "preprocessed_texts = texts.apply(preprocess_text)\n",
    "\n",
    "preprocessed_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e88ed8",
   "metadata": {},
   "source": [
    "## Data Augmentation: Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5742fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: i asked my husband to please put away the laundry and he did what he always does\n",
      "Augmented: i ask my hubby to please arrange away tatomic number 2 laundry and he did what he always behave\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert the part-of-speech naming scheme\n",
    "       from the nltk default to that which is recognized by the WordNet API\"\"\"\n",
    "    return {\n",
    "        'J': wordnet.ADJ,\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'R': wordnet.ADV\n",
    "    }.get(treebank_tag[0], wordnet.NOUN)  # Default to noun if part-of-speech is not found\n",
    "\n",
    "def synonym_replacement(sentence, num_replacements=1):\n",
    "    # Tokenize and POS tag the words in the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Get synonyms for each word, considering its part of speech\n",
    "    synonyms = defaultdict(list)\n",
    "    for word, tag in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)  # Convert to WordNet POS notation\n",
    "        for syn in wordnet.synsets(word, pos=wordnet_pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonym = lemma.name().replace('_', ' ').replace('-', ' ')\n",
    "                if synonym != word:\n",
    "                    synonyms[word].append(synonym)\n",
    "\n",
    "    # Select random words to replace\n",
    "    words_to_replace = random.sample(list(synonyms.keys()), min(num_replacements, len(synonyms)))\n",
    "\n",
    "    # Perform replacements\n",
    "    new_sentence = sentence\n",
    "    for word in words_to_replace:\n",
    "        syn_list = synonyms[word]\n",
    "        if syn_list:\n",
    "            # Choose a random synonym for the word\n",
    "            synonym = random.choice(syn_list)\n",
    "            new_sentence = new_sentence.replace(word, synonym, 1)\n",
    "\n",
    "    return new_sentence\n",
    "\n",
    "# Test the function\n",
    "original_text = \"i asked my husband to please put away the laundry and he did what he always does\"\n",
    "augmented_text = synonym_replacement(original_text, num_replacements=5)\n",
    "print(\"Original:\", original_text)\n",
    "print(\"Augmented:\", augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46a256ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def augment_sentences(dataframe, augment_factor=5):\n",
    "    augmented_rows = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        text, label = row['Texts'], row['Labels']\n",
    "        unique_augmented_texts = set()\n",
    "        while len(unique_augmented_texts) < augment_factor:\n",
    "            augmented_text = synonym_replacement(text, num_replacements=3)\n",
    "            unique_augmented_texts.add(augmented_text)\n",
    "        for aug_text in unique_augmented_texts:\n",
    "            augmented_rows.append([aug_text, label])\n",
    "    return augmented_rows\n",
    "\n",
    "\n",
    "\n",
    "augmented_data = augment_sentences(antecedents_data, augment_factor=5)\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data, columns=['Texts', 'Labels'])\n",
    "\n",
    "combined_dataset = pd.concat([antecedents_data[['Texts', 'Labels']], augmented_df])\n",
    "\n",
    "combined_dataset = combined_dataset.reset_index(drop=True)\n",
    "\n",
    "#combined_dataset.to_csv('augmented_training_data.csv', index=False)\n",
    "\n",
    "len(combined_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8338007",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63437f41",
   "metadata": {},
   "source": [
    "### 1. Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9aac687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 489 samples\n",
      "Validation set size: 123 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and Labels\n",
    "X = combined_dataset['Texts']  # the features we want to analyze\n",
    "y = combined_dataset['Labels']  # the labels, or answers, we want to test against\n",
    "\n",
    "# Split the data into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the size of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4ef66",
   "metadata": {},
   "source": [
    "### 2. Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "325e29f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training feature vectors shape: (489, 2869)\n",
      "Validation feature vectors shape: (123, 2869)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the training data to compute TF-IDF features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation data to compute TF-IDF features\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "\n",
    "# We can take a look at the shape of the resulting feature vectors\n",
    "print(f\"Training feature vectors shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Validation feature vectors shape: {X_val_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6643c940",
   "metadata": {},
   "source": [
    "### 3. Model training using Random Forest  with Multiouptout Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4321d41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9349593495934959\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the labels into a binary format for multi-label classification\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_mlb = mlb.fit_transform(y_train.apply(lambda x: x.split(',')))\n",
    "y_val_mlb = mlb.transform(y_val.apply(lambda x: x.split(',')))\n",
    "\n",
    "# Initialize the MultiOutputClassifier with RandomForest\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "multi_target_forest = MultiOutputClassifier(forest, n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "multi_target_forest.fit(X_train_tfidf, y_train_mlb)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = multi_target_forest.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val_mlb, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1184a7a6",
   "metadata": {},
   "source": [
    "### 4. Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1720b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Best hyperparameters: {'estimator__max_depth': None, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 200}\n",
      "Best cross-validated score: 0.754601226993865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [50, 100, 200],\n",
    "    'estimator__max_depth': [None, 10, 20, 30],\n",
    "    'estimator__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the MultiOutputClassifier and the parameter grid\n",
    "grid_search = GridSearchCV(multi_target_forest, param_grid=param_grid, cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_tfidf, y_train_mlb)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Best cross-validated score\n",
    "print(f\"Best cross-validated score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9777fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       precision    recall  f1-score   support\n",
      "\n",
      "                                                 busy       1.00      1.00      1.00         4\n",
      "                                                 cold       1.00      1.00      1.00         3\n",
      "                                              crowded       1.00      1.00      1.00         4\n",
      "                                               etc.)\"       1.00      1.00      1.00         3\n",
      "                                             on phone       1.00      1.00      1.00        10\n",
      "                        or challenging task/activity\"       1.00      1.00      1.00         8\n",
      "                         or overwhelming environment\"       1.00      1.00      1.00         4\n",
      "                           talking with someone else\"       1.00      1.00      1.00        10\n",
      "                                              unclear       1.00      1.00      1.00         8\n",
      "                                   \"Given a difficult       1.00      1.00      1.00         8\n",
      "                                                \"Loud       1.00      1.00      1.00         4\n",
      "\"Not really sure or \"\"out of the blue\"\" unexpectedly\"       0.00      0.00      0.00         1\n",
      "                               \"Parent/caregiver busy       1.00      1.00      1.00        10\n",
      "                            \"Physical discomfort (wet       1.00      1.00      1.00         3\n",
      "          \"They wanted something and got told \"\"no\"\"\"       1.00      1.00      1.00        13\n",
      "                 Another person had an item they want       1.00      1.00      1.00        11\n",
      "              Asked to stop using something they like       1.00      1.00      1.00         5\n",
      "                            Bad day at school or work       1.00      1.00      1.00         7\n",
      "   Belongings disturbed without permission/unexpected       1.00      0.88      0.93        16\n",
      "                            Disagreement with someone       1.00      1.00      1.00         3\n",
      "                             During class instruction       1.00      1.00      1.00         1\n",
      "             Forced participation in activity or task       1.00      1.00      1.00         1\n",
      "          I was trying to get something done in peace       1.00      1.00      1.00         5\n",
      "                              Loud or startling noise       0.00      0.00      0.00         0\n",
      "           Nobody was really paying attention to them       1.00      1.00      1.00        10\n",
      "                           Nothing to do or play with       1.00      1.00      1.00         1\n",
      "             Others were receiving a lot of attention       1.00      1.00      1.00         1\n",
      "                  Someone got in their personal space       1.00      1.00      1.00         7\n",
      "                            Someone out-competed them       1.00      1.00      1.00        11\n",
      "      Stopping one activity/task and starting another       1.00      1.00      1.00        25\n",
      "                    Sudden change of plans or routine       1.00      1.00      1.00        12\n",
      "                The kids are fighting with each other       1.00      1.00      1.00         4\n",
      "                Their choice was not honored/accepted       1.00      1.00      1.00         4\n",
      "                    They wanted something unavailable       1.00      1.00      1.00        13\n",
      "        They were asked or told to wait for something       0.50      1.00      0.67         2\n",
      "       They were asked or told to wait for something.       0.00      0.00      0.00         3\n",
      "     They were given directions or a task to complete       1.00      1.00      1.00        29\n",
      " They were in the middle of a long task or assignment       1.00      1.00      1.00         6\n",
      "They were in the middle of something they enjoy doing       1.00      1.00      1.00        21\n",
      "         While playing in a large group (like recess)       1.00      1.00      1.00         6\n",
      "                 While playing with a sibling or peer       1.00      1.00      1.00         7\n",
      "\n",
      "                                            micro avg       0.99      0.98      0.99       304\n",
      "                                            macro avg       0.91      0.92      0.92       304\n",
      "                                         weighted avg       0.98      0.98      0.98       304\n",
      "                                          samples avg       0.97      0.97      0.97       304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_forest = RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    max_depth=None, \n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Wrap the classifier with MultiOutputClassifier\n",
    "best_multi_target_forest = MultiOutputClassifier(best_forest, n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "best_multi_target_forest.fit(X_train_tfidf, y_train_mlb)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = best_multi_target_forest.predict(X_val_tfidf)\n",
    "\n",
    "# Detailed performance analysis\n",
    "print(classification_report(y_val_mlb, y_val_pred, target_names=mlb.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d725633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       precision    recall  f1-score   support\n",
      "\n",
      "                                                 busy       1.00      1.00      1.00         4\n",
      "                                                 cold       1.00      1.00      1.00         3\n",
      "                                              crowded       1.00      1.00      1.00         4\n",
      "                                               etc.)\"       1.00      1.00      1.00         3\n",
      "                                             on phone       1.00      1.00      1.00        10\n",
      "                        or challenging task/activity\"       1.00      1.00      1.00         8\n",
      "                         or overwhelming environment\"       1.00      1.00      1.00         4\n",
      "                           talking with someone else\"       1.00      1.00      1.00        10\n",
      "                                              unclear       1.00      1.00      1.00         8\n",
      "                                   \"Given a difficult       1.00      1.00      1.00         8\n",
      "                                                \"Loud       1.00      1.00      1.00         4\n",
      "\"Not really sure or \"\"out of the blue\"\" unexpectedly\"       0.00      0.00      0.00         1\n",
      "                               \"Parent/caregiver busy       1.00      1.00      1.00        10\n",
      "                            \"Physical discomfort (wet       1.00      1.00      1.00         3\n",
      "          \"They wanted something and got told \"\"no\"\"\"       1.00      1.00      1.00        13\n",
      "                 Another person had an item they want       1.00      1.00      1.00        11\n",
      "              Asked to stop using something they like       1.00      1.00      1.00         5\n",
      "                            Bad day at school or work       1.00      1.00      1.00         7\n",
      "   Belongings disturbed without permission/unexpected       1.00      0.88      0.93        16\n",
      "                            Disagreement with someone       1.00      1.00      1.00         3\n",
      "                             During class instruction       1.00      1.00      1.00         1\n",
      "             Forced participation in activity or task       1.00      1.00      1.00         1\n",
      "          I was trying to get something done in peace       1.00      1.00      1.00         5\n",
      "                              Loud or startling noise       0.00      0.00      0.00         0\n",
      "           Nobody was really paying attention to them       1.00      1.00      1.00        10\n",
      "                           Nothing to do or play with       1.00      1.00      1.00         1\n",
      "             Others were receiving a lot of attention       1.00      1.00      1.00         1\n",
      "                  Someone got in their personal space       1.00      1.00      1.00         7\n",
      "                            Someone out-competed them       1.00      1.00      1.00        11\n",
      "      Stopping one activity/task and starting another       1.00      1.00      1.00        25\n",
      "                    Sudden change of plans or routine       1.00      1.00      1.00        12\n",
      "                The kids are fighting with each other       1.00      1.00      1.00         4\n",
      "                Their choice was not honored/accepted       1.00      1.00      1.00         4\n",
      "                    They wanted something unavailable       1.00      1.00      1.00        13\n",
      "        They were asked or told to wait for something       0.50      1.00      0.67         2\n",
      "       They were asked or told to wait for something.       0.00      0.00      0.00         3\n",
      "     They were given directions or a task to complete       1.00      1.00      1.00        29\n",
      " They were in the middle of a long task or assignment       1.00      1.00      1.00         6\n",
      "They were in the middle of something they enjoy doing       1.00      1.00      1.00        21\n",
      "         While playing in a large group (like recess)       1.00      1.00      1.00         6\n",
      "                 While playing with a sibling or peer       1.00      1.00      1.00         7\n",
      "\n",
      "                                            micro avg       0.99      0.98      0.99       304\n",
      "                                            macro avg       0.91      0.92      0.92       304\n",
      "                                         weighted avg       0.98      0.98      0.98       304\n",
      "                                          samples avg       0.97      0.97      0.97       304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Since the best hyperparameters were provided, we'll use those to create a new RandomForest\n",
    "best_forest = RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    max_depth=None, \n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Wrap this forest in a MultiOutputClassifier\n",
    "best_multi_target_forest = MultiOutputClassifier(best_forest, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the full training data\n",
    "best_multi_target_forest.fit(X_train_tfidf, y_train_mlb)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = best_multi_target_forest.predict(X_val_tfidf)\n",
    "\n",
    "# Generate the classification report\n",
    "print(classification_report(y_val_mlb, y_val_pred, target_names=mlb.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa62859",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94c41296",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold: [0.926829268292683, 0.9186991869918699, 0.9262295081967213, 0.8852459016393442, 0.9180327868852459]\n",
      "Mean accuracy across all folds: 0.9150073304011729\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   8.6s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  16.4s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  16.6s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  31.9s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   8.4s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   7.8s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   9.4s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  19.2s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  28.9s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  33.8s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  16.5s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  17.4s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  25.8s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   6.5s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   6.6s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   7.2s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  18.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  32.4s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  30.6s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  16.4s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  15.6s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  25.6s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   6.6s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   6.5s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   6.9s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  15.8s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  33.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  27.6s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   7.7s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  15.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  29.7s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  32.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  13.7s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  16.6s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  31.8s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   8.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   8.2s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   7.4s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  13.6s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  26.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  27.0s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  16.4s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  15.5s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  37.6s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   8.5s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   8.2s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   8.5s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  16.4s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  30.3s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  32.7s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  13.7s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  13.0s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  32.9s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   8.5s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   9.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  16.5s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  32.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  32.7s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  21.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  14.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  32.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   8.8s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   8.2s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   8.8s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  16.6s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  27.7s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  27.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  15.8s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  19.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  29.7s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   8.4s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   7.6s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   8.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  15.8s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  27.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  25.9s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  15.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  19.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  27.6s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   8.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   8.8s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  14.7s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  14.7s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  31.7s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   7.9s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   7.7s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   7.9s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  14.7s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  33.6s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  31.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  13.7s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  14.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  25.6s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   7.0s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   7.0s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   8.2s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=100; total time=  16.0s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  34.0s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=2, estimator__n_estimators=200; total time=  34.9s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  16.6s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=100; total time=  14.6s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=5, estimator__n_estimators=200; total time=  32.1s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   8.9s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   8.1s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   7.4s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=100; total time=  13.6s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  29.9s\n",
      "[CV] END estimator__max_depth=30, estimator__min_samples_split=10, estimator__n_estimators=200; total time=  25.8s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare to collect the scores\n",
    "scores = []\n",
    "\n",
    "# Perform the cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split data\n",
    "    X_train_kf, X_val_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_kf, y_val_kf = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Transform the labels\n",
    "    y_train_kf_mlb = mlb.transform(y_train_kf.apply(lambda x: x.split(',')))\n",
    "    y_val_kf_mlb = mlb.transform(y_val_kf.apply(lambda x: x.split(',')))\n",
    "\n",
    "    # Vectorize the text\n",
    "    X_train_kf_tfidf = vectorizer.fit_transform(X_train_kf)\n",
    "    X_val_kf_tfidf = vectorizer.transform(X_val_kf)\n",
    "\n",
    "    # Train the model\n",
    "    best_multi_target_forest.fit(X_train_kf_tfidf, y_train_kf_mlb)\n",
    "\n",
    "    # Predict on the validation fold\n",
    "    y_val_kf_pred = best_multi_target_forest.predict(X_val_kf_tfidf)\n",
    "\n",
    "    # Compute the accuracy for the current fold\n",
    "    accuracy = accuracy_score(y_val_kf_mlb, y_val_kf_pred)\n",
    "\n",
    "    # Append the score\n",
    "    scores.append(accuracy)\n",
    "\n",
    "# Display the accuracy for each fold\n",
    "print(f\"Accuracy for each fold: {scores}\")\n",
    "\n",
    "# Compute the mean accuracy\n",
    "mean_accuracy = np.mean(scores)\n",
    "print(f\"Mean accuracy across all folds: {mean_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc2c0b",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb227a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlb.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained MultiOutputClassifier with RandomForest\n",
    "joblib.dump(multi_target_forest, 'antecedent_multi_target_forest.joblib')\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "\n",
    "# Save the MultiLabelBinarizer\n",
    "joblib.dump(mlb, 'mlb.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca426ad",
   "metadata": {},
   "source": [
    "## Create mapping from antecedent labels to functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3062f02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['antecedent_functions_mapping.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antecedent_functions_mapping = {\n",
    "    \"They were given directions or a task to complete\": [\"Escape\"],\n",
    "    \"They were in the middle of a long task or assignment\": [\"Escape\"],\n",
    "    \"Given a difficult, unclear, or challenging task/activity\": [\"Escape\"],\n",
    "    \"They were in the middle of something they enjoy doing\": [\"Escape\", \"Access to tangibles\"],\n",
    "    \"Someone corrected or helped them\": [\"Escape\"],\n",
    "    \"They wanted something and got told 'no'\": [\"Access to tangibles\"],\n",
    "    \"Loud, busy, crowded, or overwhelming environment\": [\"Sensory\", \"Escape\"],\n",
    "    \"Stopping one activity/task and starting another\": [\"Escape\", \"Access to tangibles\"],\n",
    "    \"While playing with a sibling or peer\": [\"Escape\", \"Access to tangibles\", \"Attention\", \"Sensory\"],\n",
    "    \"While playing in a large group (like recess)\": [\"Escape\", \"Access to tangibles\", \"Attention\", \"Sensory\"],\n",
    "    \"Nobody was really paying attention to them\": [\"Attention\"],\n",
    "    \"Nothing to do or play with\": [\"Escape\", \"Access to tangibles\", \"Attention\", \"Sensory\"],\n",
    "    \"Others were receiving a lot of attention\": [\"Attention\"],\n",
    "    \"Parent/caregiver busy, on phone, talking with someone else\": [\"Attention\"],\n",
    "    \"Sudden change of plans or routine\": [\"Escape\", \"Access to tangibles\", \"Attention\"],\n",
    "    \"They wanted something unavailable\": [\"Escape\", \"Access to tangibles\"],\n",
    "    \"Another person had an item they want\": [\"Access to tangibles\"],\n",
    "    \"Belongings disturbed without permission/unexpected\": [\"Access to tangibles\"],\n",
    "    \"Physical discomfort (wet, cold, etc.)\": [\"Sensory\", \"Escape\"],\n",
    "    \"Asked to stop using something they like\": [\"Access to tangibles\", \"Escape\"],\n",
    "    \"Disagreement with someone\": [\"Access to tangibles\", \"Escape\", \"Attention\"],\n",
    "    \"Someone out-competed them\": [\"Access to tangibles\", \"Escape\"],\n",
    "    \"Forced participation in activity or task\": [\"Escape\"],\n",
    "    \"Their choice was not honored/accepted\": [\"Access to tangibles\"],\n",
    "    \"Someone got in their personal space\": [\"Escape\"],\n",
    "    \"Bright or fluorescent lights\": [\"Sensory\", \"Escape\"],\n",
    "    \"Loud or startling noise\": [\"Sensory\", \"Escape\"],\n",
    "    \"Someone raised their voice\": [\"Sensory\", \"Escape\"],\n",
    "    \"They were touched without giving permission\": [\"Escape\"],\n",
    "    \"Someone purposely antagonized them\": [\"Escape\"],\n",
    "    \"Someone deceived or tricked them\": [\"Escape\", \"Access to tangibles\"],\n",
    "    \"Peer pressure to fit in or impress others\": [\"Escape\", \"Attention\"],\n",
    "    \"Bad day at school or work\": [\"Escape\", \"Access to tangibles\"],\n",
    "    \"Not really sure or 'out of the blue' unexpectedly\": [\"Sensory\", \"Attention\", \"Escape\", \"Access to tangibles\"],\n",
    "    \"During class instruction\": [\"Escape\", \"Attention\"],\n",
    "    \"The kids are fighting with each other\": [\"Escape\"],\n",
    "    \"I was trying to get something done in peace\": [\"Access to tangibles\"],\n",
    "    \"Fighting/disagreement with partner\": [\"Access to tangibles\", \"Escape\", \"Attention\"],\n",
    "    \"They were asked or told to wait for something.\": [\"Access to tangibles\", \"Attention\", \"Escape\"]\n",
    "}\n",
    "\n",
    "\n",
    "joblib.dump(antecedent_functions_mapping, 'antecedent_functions_mapping.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ed61d",
   "metadata": {},
   "source": [
    "## For future use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd20158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and components\n",
    "loaded_model = joblib.load('antecedent_multi_target_forest.joblib')\n",
    "loaded_vectorizer = joblib.load('vectorizer.joblib')\n",
    "loaded_mlb = joblib.load('mlb.joblib')\n",
    "loaded_functions_mapping = joblib.load('antecedent_functions_mapping.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6a468",
   "metadata": {},
   "source": [
    "### Now we can use these loaded objects to preprocess input, predict labels, and map them to functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d57d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is just an example\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the saved model and other components\n",
    "multi_target_forest = joblib.load('antecedent_multi_target_forest.joblib')\n",
    "vectorizer = joblib.load('vectorizer.joblib')\n",
    "mlb = joblib.load('mlb.joblib')\n",
    "antecedent_functions_mapping = joblib.load('antecedent_functions_mapping.joblib')\n",
    "\n",
    "# Example function to retrieve functions based on antecedents\n",
    "def get_functions_from_antecedents(antecedent_labels):\n",
    "    all_functions = [antecedent_functions_mapping[label] \n",
    "                     for label in antecedent_labels if label in antecedent_functions_mapping]\n",
    "    unique_functions = list(set(sum(all_functions, [])))\n",
    "    return unique_functions\n",
    "\n",
    "# Example prediction function\n",
    "def predict(input_text, threshold=0.3):\n",
    "    preprocessed_text = preprocess_text(input_text)\n",
    "    #print(f\"Preprocessed text: '{preprocessed_text}'\")\n",
    "    tfidf_features = vectorizer.transform([preprocessed_text])\n",
    "    \n",
    "    probabilities = multi_target_forest.predict_proba(tfidf_features)\n",
    "    proba_positive_class = np.array([prob[:, 1] for prob in probabilities]).T\n",
    "    #print(f\"Probabilities: {proba_positive_class}\")\n",
    "\n",
    "    binary_predictions = (proba_positive_class > threshold).astype(int)\n",
    "    #print(f\"Binary predictions shape: {binary_predictions.shape}\")\n",
    "\n",
    "    # Check if at least one label meets the threshold\n",
    "    if not binary_predictions.any():\n",
    "        # If no labels meet the threshold, take the label(s) with the highest probability below the threshold\n",
    "        max_proba_idx = np.argmax(proba_positive_class, axis=1)\n",
    "        binary_predictions[0, max_proba_idx] = 1\n",
    "    \n",
    "    antecedent_labels = mlb.inverse_transform(binary_predictions)\n",
    "    \n",
    "    functions = get_functions_from_antecedents(antecedent_labels[0]) if antecedent_labels else []\n",
    "    \n",
    "    return {\n",
    "        \"antecedent_labels\": antecedent_labels[0] if antecedent_labels else (),\n",
    "        \"functions\": functions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0b30941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antecedent_labels': ('They were in the middle of something they enjoy doing',), 'functions': ['Escape', 'Access to tangibles']}\n",
      "{'antecedent_labels': (' on phone', ' talking with someone else\"', '\"Parent/caregiver busy', 'Nobody was really paying attention to them', 'They were in the middle of a long task or assignment'), 'functions': ['Escape', 'Attention']}\n",
      "{'antecedent_labels': ('Someone got in their personal space', 'While playing with a sibling or peer'), 'functions': ['Sensory', 'Escape', 'Access to tangibles', 'Attention']}\n"
     ]
    }
   ],
   "source": [
    "# Now simulate a prediction\n",
    "\n",
    "input_text1 = \"i asked my husband to please put away the laundry and he did what he always does\"\n",
    "input_text2 = \"Teacher is grading papers and the class is working independently\"\n",
    "input_text3 = \"he walk into my bedroom without knocking door and sit near me\"\n",
    "\n",
    "# Test the prediction function with different texts\n",
    "print(predict(input_text1))\n",
    "print(predict(input_text2))\n",
    "print(predict(input_text3))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
